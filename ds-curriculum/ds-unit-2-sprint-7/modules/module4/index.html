<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DS7 Module 4 - Model Interpretation</title>
    <link rel="stylesheet" href="../../assets/css/style.css">
</head>
<body>
    <header>
        <nav>
            <div class="logo">Data Science Unit 2</div>
            <ul>
                <li><a href="../../index.html">Home</a></li>
                <li class="dropdown">
                    <a href="#" class="active">Modules</a>
                    <div class="dropdown-content">
                        <a href="../module1/index.html">Module 1: Define ML Problems</a>
                        <a href="../module2/index.html">Module 2: Wrangle ML Datasets</a>
                        <a href="../module3/index.html">Module 3: Permutation and Boosting</a>
                        <a href="../module4/index.html" class="active">Module 4: Model Interpretation</a>
                    </div>
                </li>
                <li class="dropdown">
                    <a href="#">Code-Alongs</a>
                    <div class="dropdown-content">
                        <a href="../../code-alongs/index.html#code-along-1">Code-Along 1: Feature Engineering</a>
                        <a href="../../code-alongs/index.html#code-along-2">Code-Along 2: Model Interpretation</a>
                    </div>
                </li>
                <li><a href="../../sprint-challenge/index.html">Sprint Challenge</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <section id="welcome">
            <h1>Module 4: Model Interpretation</h1>
            <div class="content-box module4-accent">
                <p>In this final module of the sprint, you'll learn techniques for interpreting machine learning models and explaining their predictions. Model interpretability is crucial for building stakeholder trust, ensuring ethical decision-making, debugging models, and gaining insights into your data that you can communicate effectively.</p>
            </div>
        </section>

        <section>
            <h2>Learning Objectives</h2>
            
            <div class="content-box">
                <h3>1. Model Interpretability</h3>
                <p>Learn the importance of model interpretability and techniques for making models more transparent and understandable.</p>
                <ul>
                    <li>Understanding the trade-off between model complexity and interpretability</li>
                    <li>Learning different types of model interpretability approaches (intrinsic vs post-hoc)</li>
                    <li>Identifying situations where interpretability is critical</li>
                    <li>Comparing global vs local interpretation methods</li>
                    <li>Understanding the limitations of black-box models</li>
                    <li>Implementing model-agnostic interpretation techniques</li>
                </ul>
            </div>
            
            <div class="content-box">
                <h3>2. Visualize and interpret PDP plots</h3>
                <p>Learn how to create and interpret Partial Dependence Plots (PDP) to understand how features affect model predictions.</p>
                <ul>
                    <li>Understanding what partial dependence plots reveal about feature effects</li>
                    <li>Creating PDPs using interpretation libraries like PDPBox</li>
                    <li>Interpreting single-feature and two-feature interaction PDPs</li>
                    <li>Using PDPs to guide feature engineering decisions</li>
                    <li>Identifying non-linear relationships between features and predictions</li>
                    <li>Understanding the limitations of PDPs</li>
                </ul>
            </div>
            
            <div class="content-box">
                <h3>3. Explain individual predictions with shapley value plots</h3>
                <p>Learn how to use SHAP (SHapley Additive exPlanations) values to explain individual predictions and understand feature contributions.</p>
                <ul>
                    <li>Understanding the mathematical principles behind Shapley values</li>
                    <li>Implementing SHAP for different model types</li>
                    <li>Creating and interpreting force plots for individual predictions</li>
                    <li>Using SHAP to identify feature importance globally and locally</li>
                    <li>Comparing SHAP with other feature importance methods</li>
                    <li>Communicating model decisions effectively using SHAP visualizations</li>
                </ul>
            </div>
        </section>

        <section>
            <h2>Guided Project</h2>
            <div class="content-box">
                <h3>Model Interpretation Guided Project</h3>
                <div class="video-container">
                    <iframe src="https://fast.wistia.net/embed/iframe/dstvalsfjl" title="Model Interpretation - Part One" allowtransparency="true" frameborder="0" scrolling="no" class="wistia_embed" name="wistia_embed" allowfullscreen mozallowfullscreen webkitallowfullscreen oallowfullscreen msallowfullscreen></iframe>
                </div>
                
                <h3>Part Two Video</h3>
                <div class="video-container">
                    <iframe src="https://fast.wistia.net/embed/iframe/zugvd8mxx2" title="Model Interpretation - Part Two" allowtransparency="true" frameborder="0" scrolling="no" class="wistia_embed" name="wistia_embed" allowfullscreen mozallowfullscreen webkitallowfullscreen oallowfullscreen msallowfullscreen></iframe>
                </div>
                
                <p>In this guided project, you'll work through a complete workflow for model interpretation, using techniques like partial dependence plots and SHAP values to understand how your model makes predictions and which features have the greatest impact.</p>
                
                <div class="resource-links">
                    <a href="https://github.com/bloominstituteoftechnology/DS-Unit-2-Applied-Modeling/tree/master/module4-model-interpretation" class="resource-link" target="_blank" rel="noopener">GitHub Repository</a>
                    <a href="#" class="resource-link" target="_blank" rel="noopener">LS_DS_234.ipynb</a>
                    <a href="#" class="resource-link" target="_blank" rel="noopener">DS_234_guided_project_notes.ipynb</a>
                </div>
            </div>
        </section>

        <section>
            <h2>Module Assignment</h2>
            <div class="content-box">
                <h3>Model Interpretation for Your Portfolio Project</h3>
                <p>For this final assignment, you'll apply model interpretation techniques to your portfolio project to gain insights and effectively communicate your model's behavior.</p>
                
                <p><em>Note: There is no video for this assignment as you will be working with your own dataset and defining your own machine learning problem.</em></p>
                
                <h4>Assignment Notebook Name: LS_DS_234_assignment.ipynb</h4>
                <h4>Tasks:</h4>
                <ol>
                    <li>Continue to iterate on your project: data cleaning, exploratory visualization, feature engineering, modeling.</li>
                    <li>Make at least 1 partial dependence plot to explain your model.</li>
                    <li>Make at least 1 Shapley force plot to explain an individual prediction.</li>
                </ol>
                
                <div class="resource-links">
                    <a href="https://github.com/bloominstituteoftechnology/DS-Unit-2-Applied-Modeling/tree/master/module4-model-interpretation" class="resource-link" target="_blank" rel="noopener">Assignment GitHub Repository</a>
                </div>
            </div>
        </section>
        
        <section>
            <h2>Additional Resources</h2>
            <div class="content-box">
                <div class="resource-card">
                    <h3>Model Validation</h3>
                    <ul>
                        <li><a href="https://scikit-learn.org/stable/modules/cross_validation.html" target="_blank" rel="noopener">Scikit-learn Cross-validation Guide</a></li>
                        <li><a href="https://scikit-learn.org/stable/modules/learning_curve.html" target="_blank" rel="noopener">Learning Curve Documentation</a></li>
                        <li><a href="https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/" target="_blank" rel="noopener">Evaluation Metrics for Imbalanced Classification</a></li>
                    </ul>
                </div>
                
                <div class="resource-card">
                    <h3>Hyperparameter Tuning</h3>
                    <ul>
                        <li><a href="https://scikit-learn.org/stable/modules/grid_search.html" target="_blank" rel="noopener">Scikit-learn Tuning Guide for Estimators</a></li>
                        <li><a href="https://github.com/microsoft/nni" target="_blank" rel="noopener">Microsoft's Neural Network Intelligence (NNI) Toolkit</a></li>
                        <li><a href="https://optuna.org/" target="_blank" rel="noopener">Optuna Hyperparameter Optimization Framework</a></li>
                    </ul>
                </div>
                
                <div class="resource-card">
                    <h3>Data Visualization and Communication</h3>
                    <ul>
                        <li><a href="https://matplotlib.org/stable/gallery/index.html" target="_blank" rel="noopener">Matplotlib Gallery</a></li>
                        <li><a href="https://seaborn.pydata.org/examples/index.html" target="_blank" rel="noopener">Seaborn Example Gallery</a></li>
                    </ul>
                </div>
            </div>
        </section>
    </main>
</body>
</html> 