<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 1: Natural Language Processing - Introduction</title>
    <link rel="stylesheet" href="../../assets/css/style.css">
</head>
<body>
    <div class="container">
        <header>
            <nav>
                <div class="logo">Data Science Unit 4</div>
                <ul>
                    <li><a href="../../index.html" class="active">Home</a></li>
                    <li class="dropdown">
                        <a href="#">Modules</a>
                        <div class="dropdown-content">
                            <a href="../module1/index.html">Module 1: Natural Language Processing - Introduction</a>
                            <a href="../module2/index.html">Module 2: Vector Representations</a>
                            <a href="../module3/index.html">Module 3: Document Classification</a>
                            <a href="../module4/index.html">Module 4: Topic Modeling</a>
                        </div>
                    </li>
                    <li><a href="../../code-alongs/index.html">Code-Alongs</a></li>
                    <li><a href="../../sprint-challenge/index.html">Sprint Challenge</a></li>
                </ul>
            </nav>
        </header>
        
        <main>
            <h1>Module 1: Natural Language Processing - Introduction</h1>
            <section id="module-overview">
                <h2>Module Overview</h2>
                <p>Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language. In this module, we'll explore the foundational concepts of NLP, including text preprocessing techniques that are essential for any text-based analysis. We'll learn how to tokenize text, remove stop words, and apply stemming or lemmatization to prepare text data for more advanced NLP applications.</p>
            </section>

            <section id="learning-objectives">
                <h2>Learning Objectives</h2>
                
                <h3>1. Tokenize Text</h3>
                <p>
                    • Break text into individual tokens (words, sentences, etc.)<br>
                    • Apply different tokenization methods (character, word, sentence)<br>
                    • Use Python libraries for efficient tokenization<br>
                    • Handle punctuation and special characters during tokenization
                </p>

                <h3>2. Remove Stop Words From a List of Tokens</h3>
                <p>
                    • Identify common stop words in text<br>
                    • Remove stop words to improve analysis efficiency<br>
                    • Use pre-defined stop word lists<br>
                    • Create custom stop word lists for domain-specific applications
                </p>

                <h3>3. Stem or Lemmatize Text</h3>
                <p>
                    • Apply stemming algorithms to reduce words to their root form<br>
                    • Implement lemmatization to find the base dictionary form of words<br>
                    • Compare stemming vs. lemmatization approaches<br>
                    • Choose appropriate text normalization methods for different NLP tasks
                </p>
            </section>

            <section id="guided-project">
                <h2>Guided Project</h2>
                <h3>NLP Introduction: Text Preprocessing</h3>
                
                <div class="video-container">
                    <iframe class="wistia_embed" title="Sprint 13 Natural Language Processing - Introduction Video" src="https://fast.wistia.net/embed/iframe/1gg4tasbei" width="640" height="360" name="wistia_embed" allow="autoplay; fullscreen" loading="lazy"></iframe>
                </div>
                
                <p>
                    <a href="https://github.com/bloominstituteoftechnology/DS-Unit-4-Sprint-1-NLP/tree/main/module1-text-data" target="_blank" rel="noopener">GitHub Repo</a> | 
                    <a href="https://docs.google.com/presentation/d/1-55ghoTUn6saUFF_tHllanhYhVA9F6YLcRtkdJWSspg/edit?usp=sharing" target="_blank" rel="noopener">Slides</a>
                </p>

                <h3>Guided Project File:</h3>
                <p>DS_411_Text_Data_Lecture_GP.ipynb</p>
            </section>

            <section id="module-assignment">
                <h2>Module Assignment</h2>
                <p>Please read the assignment file in the GitHub repository for detailed instructions on completing your assignment tasks.</p>
                
                <h3>Assignment File:</h3>
                <p>DS_411_Text_Data_Assignment.ipynb</p>

                <p>In this assignment, your goal is to find the attributes of the best & worst coffee shops in the dataset. The text is fairly raw: dates in the review, extra words in the star_rating column, etc. You'll probably want to clean that stuff up for a better analysis.</p>
                
                <p>You will need to analyze the corpus of text using text visualizations of token frequency. Try cleaning the data as much as possible using techniques including:</p>
                <ul>
                    <li>Lemmatization</li>
                    <li>Custom stopword removal</li>
                </ul>

                <h3>Assignment Solution Video</h3>
                <div class="video-container">
                    <iframe class="wistia_embed" title="DS_411_Text_Data_Assignment - Jupyter Notebook Video" src="https://fast.wistia.net/embed/iframe/szu5dzag83" width="640" height="360" name="wistia_embed" allow="autoplay; fullscreen" loading="lazy"></iframe>
                </div>
            </section>

            <section id="check-for-understanding">
                <h2>Check for Understanding</h2>
                <p>Complete the following items to test your understanding:</p>
                <ul>
                    <li>Tokenize a text document using different methods (word, sentence)</li>
                    <li>Remove standard and custom stop words from tokenized text</li>
                    <li>Apply stemming and lemmatization to a set of tokens</li>
                    <li>Compare the results of stemming vs. lemmatization</li>
                    <li>Create a visualization of token frequencies</li>
                </ul>
            </section>

            <section id="additional-resources">
                <h2>Additional Resources</h2>
                <ul>
                    <li><a href="https://spacy.io/usage/linguistic-features" target="_blank" rel="noopener">spaCy Linguistic Features Documentation</a></li>
                    <li><a href="https://www.nltk.org/book/ch03.html" target="_blank" rel="noopener">NLTK Book: Processing Raw Text</a></li>
                    <li><a href="https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction" target="_blank" rel="noopener">Scikit-Learn: Text Feature Extraction</a></li>
                    <li><a href="https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908" target="_blank" rel="noopener">Text Preprocessing in Python: Steps, Tools, and Examples</a></li>
                </ul>
            </section>
        </main>
    </div>
</body>
</html>
