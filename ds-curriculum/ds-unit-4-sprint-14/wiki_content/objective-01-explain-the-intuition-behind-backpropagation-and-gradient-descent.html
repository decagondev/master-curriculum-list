<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<title>Objective 01 - Explain the Intuition Behind Backpropagation and Gradient Descent</title>
<meta name="identifier" content="ge836f10cafd0662d5e178f515c365b97"/>
<meta name="editing_roles" content="teachers"/>
<meta name="workflow_state" content="active"/>
</head>
<body>
<h2>Overview</h2>
<p>In the previous module, we learned more about the main components of neural networks, including neurons, weights, activation function, and the types of layers. We also created a simple perceptron binary classifier and an actual neural network using the Keras library. As we built those models, we focused more on the architecture, such as the number of layers and neurons in those layers. We also looked at parameters like the batch size and the learning rate and how they affect training time.</p>
<p>Now we're going to get more specific about how a neural network learns. First, we need to specify that we're talking about a feed-forward neural network, where information moves forward from the input. The weights are adjusted for each iteration; this process doesn't transfer data from one layer to the next inside the neural network.</p>
<p>In contrast, consider a multi-layer neural network where this transfer does occur. For example, if we added feedback from the last hidden layer to the first hidden layer, it would be considered a <em>recurrent neural network</em> (RNN), something we will cover in the next sprint.</p>
<h3>Loss Function</h3>
<p>So, what's going on when we train a neural network? First, remember to adjust the weights by comparing the model prediction with the expected result and adding or subtracting to those values to get closer to the correct answer. To do that, we minimize a loss function that compares the output to the target (correct answer). Using the process of gradient descent will minimize this function.</p>
<h3>Gradient Descent</h3>
<p>Given a function, in this case, the neural network loss function, we find the minimum of the function with the process of gradient descent. We start by choosing a random location on the function, finding the negative gradient's direction, and then repeat the process to find a minimum of the function. Sometimes, we find a local minimum, but the goal is to find the global minimum.</p>
<h2>Follow Along</h2>
<p>Let's work through an example with an actual function and find the minimum using gradient descent. The function we'll use is a simple parabola that has a single minimum.</p>
<p><img src="https://i.upmath.me/svg/%20y%20%3D%20(x-3)%5E2%20" alt="y = (x-3)^2" loading="lazy"></p>
<p>Let's graph this function to see if we can see visually where the minimum is.</p>
<div class="codehilite">
<pre><code><span class="c1"># Plot the function</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'x'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'y'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>
<p><img src="https://raw.githubusercontent.com/bloominstituteoftechnology/data-science-canvas-images/main/unit_4/sprint_2/mod2_obj1_function.png" alt="mod2<em>obj1</em>function.png" loading="lazy"></p>
<p>We can see it reaches a minimum (y=0) when x=3. Using calculus, we can find the minimum mathematically. We do this by looking at the slope of the function and finding when that slope is equal to zero. We find the slope by taking the derivative of the function. We'll take the derivative of y for x.</p>
<p>If we set the derivative equal to zero and solve for <img src="https://i.upmath.me/svg/x" alt="x" loading="lazy">, we’ll have a solution for the minimum of the function:</p>
<p><img src="https://i.upmath.me/svg/%200%20%3D%202(x-3)" alt="0 = 2(x-3)" loading="lazy"> which is true when x = 3.</p>
<p>Using Python, we’ll implement a gradient descent function. First let’s rewrite the equation as follows:</p>
<p><img src="https://i.upmath.me/svg/%20x_%7Bcurrent%7D%20%3D%20(x_%7Bprevious%7D%20-%20%5Ctext%7Blearning%20rate%7D)%20*%20%5Cfrac%7Bdx%7D%7Bdy%7D" alt="x<em>{current} = (x</em>{previous} - \text{learning rate}) * \frac{dx}{dy}" loading="lazy"></p>
<p>where <img src="https://i.upmath.me/svg/%5Cfrac%7Bdx%7D%7Bdy%7D" alt="\frac{dx}{dy}" loading="lazy"> equals <img src="https://i.upmath.me/svg/2(x_%7Bprevious%7D-3)" alt="2(x_{previous}-3)" loading="lazy">.</p>
<p>The basic steps that we'll follow are:</p>
<ul>
<li>initialize at a value for x</li>
<li>calculate a "new" current <em>x</em> with the learning rate and gradient</li>
<li>update the next iteration with this "new" value of x</li>
<li>iterate until the maximum number of iterations is reached</li>
</ul>
<div class="codehilite">
<pre><code><span class="c1"># Initialize at x=1</span>
<span class="n">cur_x</span> <span class="o">=</span> <span class="mi">1</span> 

<span class="c1"># Learning rate (how much to adjust x each iteration)</span>
<span class="n">rate</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="c1"># Maximum number of iterations</span>
<span class="n">max_iters</span> <span class="o">=</span> <span class="mi">25</span>

<span class="c1"># Initialize interation counter</span>
<span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Gradient of the function</span>
<span class="n">grad</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>

<span class="k">while</span> <span class="n">iters</span> <span class="o">&lt;</span> <span class="n">max_iters</span><span class="p">:</span>
  <span class="c1"># Set the previous x as the current</span>
  <span class="n">prev_x</span> <span class="o">=</span> <span class="n">cur_x</span>

  <span class="c1"># Calculate the "new" current x with the gradient</span>
  <span class="n">cur_x</span> <span class="o">=</span> <span class="n">prev_x</span> <span class="o">-</span> <span class="p">(</span><span class="n">rate</span> <span class="o">*</span> <span class="n">grad</span><span class="p">(</span><span class="n">prev_x</span><span class="p">))</span>

  <span class="c1"># Advance the iteration counter</span>
  <span class="n">iters</span> <span class="o">=</span> <span class="n">iters</span><span class="o">+</span><span class="mi">1</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Iteration </span><span class="si">{</span><span class="n">iters</span><span class="si">}</span><span class="s2"> - x value: </span><span class="si">{</span><span class="n">cur_x</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># Print out the final result</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"The local minimum occurs at"</span><span class="p">,</span> <span class="n">cur_x</span><span class="p">)</span>
</code></pre>
</div>
<pre><code>Iteration 1 - x value: 1.2
Iteration 2 - x value: 1.38
Iteration 3 - x value: 1.5419999999999998
Iteration 4 - x value: 1.6877999999999997
Iteration 5 - x value: 1.8190199999999999
Iteration 6 - x value: 1.937118
Iteration 7 - x value: 2.0434061999999997
Iteration 8 - x value: 2.1390655799999996
Iteration 9 - x value: 2.2251590219999997
Iteration 10 - x value: 2.3026431198
Iteration 11 - x value: 2.37237880782
Iteration 12 - x value: 2.4351409270380002
Iteration 13 - x value: 2.4916268343342
Iteration 14 - x value: 2.54246415090078
Iteration 15 - x value: 2.5882177358107024
Iteration 16 - x value: 2.629395962229632
Iteration 17 - x value: 2.6664563660066687
Iteration 18 - x value: 2.6998107294060016
Iteration 19 - x value: 2.7298296564654017
Iteration 20 - x value: 2.7568466908188616
Iteration 21 - x value: 2.7811620217369755
Iteration 22 - x value: 2.8030458195632777
Iteration 23 - x value: 2.82274123760695
Iteration 24 - x value: 2.840467113846255
Iteration 25 - x value: 2.8564204024616293
The local minimum occurs at 2.8564204024616293
</code></pre>
<p>After 25 iterations, the value is starting to converge on the minimum of x=3. To speed up the convergence, we can adjust the learning rate or the size of the step we adjust <em>x</em> by. We'll leave that up to you to complete as a challenge.</p>
<h2>Challenge</h2>
<p>Following the example above, try experimenting with the gradient descent algorithm. Specifically, you can change the learning rate and adjust the maximum number of iterations. How close can you get to the minimum value of the function?</p>
<h2>Additional Resources</h2>
<ul>
<li><a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/why-the-gradient-is-the-direction-of-steepest-ascent">Khan Academy: Gradient and Directional Derivatives</a></li>
<li><a href="https://towardsdatascience.com/implement-gradient-descent-in-python-9b93ed7108d1">Implement a Gradient Descent in Python</a></li>
</ul>
</body>
</html>