<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 2: Train</title>
    <link rel="stylesheet" href="../../assets/css/styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Module 2: Train</h1>
            <nav>
                <ul>
                    <li><a href="../../index.html">Home</a></li>
                    <li><a href="#module-overview">Module Overview</a></li>
                    <li><a href="#learning-objectives">Learning Objectives</a></li>
                    <li><a href="#guided-project">Guided Project</a></li>
                    <li><a href="#module-assignment">Module Assignment</a></li>
                    <li><a href="#check-for-understanding">Check for Understanding</a></li>
                    <li><a href="#additional-resources">Additional Resources</a></li>
                </ul>
            </nav>
        </header>

        <main>
            <section id="module-overview">
                <h2>Module Overview</h2>
                <div class="content-box">
                    <p>This module focuses on training neural networks effectively using gradient descent and backpropagation algorithms. You'll learn the fundamental principles behind how neural networks learn from data, including the optimization process that adjusts weights and biases to minimize loss. The module also explores critical hyperparameters like batch size and learning rate that significantly impact model performance and convergence.</p>
                </div>
            </section>

            <section id="learning-objectives">
                <h2>Learning Objectives</h2>
                <div class="content-box">
                    <h3>1. Explain the intuition behind backpropagation and gradient descent</h3>
                    <ul>
                        <li>Understand how gradient descent optimizes the loss function</li>
                        <li>Explain the chain rule's role in backpropagation</li>
                        <li>Differentiate between types of gradient descent (batch, stochastic, mini-batch)</li>
                        <li>Visualize the optimization process across a loss landscape</li>
                        <li>Understand the role of loss functions in training and model evaluation</li>
                    </ul>
                </div>

                <div class="content-box">
                    <h3>2. Understand the role and importance of batch size</h3>
                    <ul>
                        <li>Define what batch size means in neural network training</li>
                        <li>Analyze how batch size affects training speed and memory usage</li>
                        <li>Examine the impact of batch size on model convergence</li>
                        <li>Implement strategies for selecting optimal batch sizes</li>
                    </ul>
                </div>

                <div class="content-box">
                    <h3>3. Understand the role and importance of learning rate</h3>
                    <ul>
                        <li>Define the learning rate parameter and its function</li>
                        <li>Analyze how learning rate affects convergence speed and stability</li>
                        <li>Compare different optimizers and their learning rate sensitivity</li>
                        <li>Diagnose and address common issues related to learning rate selection</li>
                    </ul>
                </div>
            </section>

            <section id="guided-project">
                <h2>Guided Project</h2>
                <div class="content-box">
                    <h3>Training Neural Networks with Keras</h3>
                    
                    <div class="video-container">
                        <iframe class="wistia_embed" title="Sprint 14 Neural Network Foundations - Training Neural Networks Video" src="https://fast.wistia.net/embed/iframe/1fbojf6oid" width="640" height="360" name="wistia_embed" allow="autoplay; fullscreen" loading="lazy"></iframe>
                    </div>
                    
                    <div class="resource-links">
                        <a href="https://github.com/bloominstituteoftechnology/DS-Unit-4-Sprint-2-Neural-Networks/tree/main/module2-Train" class="resource-link primary" target="_blank">GitHub Repo</a>
                        <a href="https://docs.google.com/presentation/d/1LsNqBJXI_pvdqGn5ZcG6AfmXhUN65jvTd_v6Ua8HC5w/edit?usp=sharing" target="_blank">Slides</a>
                    </div>
                    
                    <h4>Guided Project File:</h4>
                    <p>DS_422_Train_Lecture.ipynb</p>
                </div>
            </section>

            <section id="module-assignment">
                <h2>Module Assignment</h2>
                <div class="content-box">
                    <p>Please read the assignment file in the GitHub repository for detailed instructions on completing your assignment tasks.</p>
                    
                    <h4>Assignment File:</h4>
                    <p>DS_422_Train_Assignment.ipynb</p>

                    <p>In this assignment, you will continue to build a sketch classification model using the Quickdraw dataset. Your tasks include:</p>
                    <ul>
                        <li>Comparing model performance with normalized vs. non-normalized data</li>
                        <li>Implementing a neural network with specific architecture requirements</li>
                        <li>Running experiments with different batch sizes to analyze their impact</li>
                        <li>Testing various learning rates and comparing their effects on model convergence</li>
                        <li>Experimenting with different optimizers (SGD, Adam) and analyzing results</li>
                        <li>Visualizing and interpreting weight distributions using TensorBoard</li>
                    </ul>

                    <h3>Assignment Solution Video</h3>
                    <div class="video-container">
                        <iframe class="wistia_embed" title="DS_422_Train_Assignment - Jupyter Notebook Video" src="https://fast.wistia.net/embed/iframe/xe1xe23znp" width="640" height="360" name="wistia_embed" allow="autoplay; fullscreen" loading="lazy"></iframe>
                    </div>
                </div>
            </section>

            <section id="check-for-understanding">
                <h2>Check for Understanding</h2>
                <div class="content-box">
                    <p>Complete the following items to test your understanding:</p>
                    <ul>
                        <li>Explain the key steps of the backpropagation algorithm and its relationship to gradient descent</li>
                        <li>Describe how different batch sizes affect neural network training and when to choose larger or smaller batches</li>
                        <li>Explain the consequences of setting the learning rate too high or too low</li>
                        <li>Compare and contrast different optimization algorithms (SGD, Adam) and their use cases</li>
                        <li>Explain why data normalization is important for neural network training</li>
                        <li>Describe common loss functions and when to use each type (MSE, categorical cross-entropy, etc.)</li>
                    </ul>
                </div>
            </section>

            <section id="additional-resources">
                <h2>Additional Resources</h2>
                <div class="content-box">
                    <ul>
                        <li><a href="https://www.youtube.com/watch?v=IHZwWFHWa-w" target="_blank">3Blue1Brown: Gradient Descent, How Neural Networks Learn</a></li>
                        <li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers" target="_blank">TensorFlow: Keras Optimizers Documentation</a></li>
                        <li><a href="https://www.tensorflow.org/tensorboard/get_started" target="_blank">Getting Started with TensorBoard</a></li>
                        <li><a href="https://distill.pub/2017/momentum/" target="_blank">Distill: Why Momentum Really Works</a></li>
                        <li><a href="https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/" target="_blank">How to Configure the Learning Rate When Training Deep Learning Neural Networks</a></li>
                        <li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses" target="_blank">TensorFlow: Keras Loss Functions Documentation</a></li>
                    </ul>
                </div>
            </section>
        </main>

        <footer>
            <p>&copy; 2025 DS Unit 4 Sprint 14 - Module 2</p>
        </footer>
    </div>
</body>
</html>
