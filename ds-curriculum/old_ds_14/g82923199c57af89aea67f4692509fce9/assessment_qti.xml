<?xml version="1.0" encoding="UTF-8"?>
<questestinterop xmlns="http://www.imsglobal.org/xsd/ims_qtiasiv1p2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.imsglobal.org/xsd/ims_qtiasiv1p2 http://www.imsglobal.org/profile/cc/ccv1p1/ccv1p1_qtiasiv1p2p1_v1p0.xsd">
  <assessment ident="g82923199c57af89aea67f4692509fce9" title="Deploy Check For Understanding">
    <qtimetadata>
      <qtimetadatafield>
        <fieldlabel>cc_profile</fieldlabel>
        <fieldentry>cc.exam.v0p1</fieldentry>
      </qtimetadatafield>
      <qtimetadatafield>
        <fieldlabel>qmd_assessmenttype</fieldlabel>
        <fieldentry>Examination</fieldentry>
      </qtimetadatafield>
      <qtimetadatafield>
        <fieldlabel>qmd_scoretype</fieldlabel>
        <fieldentry>Percentage</fieldentry>
      </qtimetadatafield>
      <qtimetadatafield>
        <fieldlabel>cc_maxattempts</fieldlabel>
        <fieldentry>unlimited</fieldentry>
      </qtimetadatafield>
    </qtimetadata>
    <section ident="root_section">
      <item ident="gebc99d3d6257206d6c485060ab2fe9e4" title="8450f20b-ec09-4d4e-86c4-5b4db030e739">
        <itemmetadata>
          <qtimetadata>
            <qtimetadatafield>
              <fieldlabel>cc_profile</fieldlabel>
              <fieldentry>cc.multiple_choice.v0p1</fieldentry>
            </qtimetadatafield>
          </qtimetadata>
        </itemmetadata>
        <presentation>
          <material>
            <mattext texttype="text/html">&lt;div&gt;Which of the following regularization techniques determines which neurons receive data from the previous layer?&lt;/div&gt;</mattext>
          </material>
          <response_lid ident="response1" rcardinality="Single">
            <render_choice>
              <response_label ident="3440">
                <material>
                  <mattext texttype="text/plain">dropout</mattext>
                </material>
              </response_label>
              <response_label ident="6233">
                <material>
                  <mattext texttype="text/plain">weight decay</mattext>
                </material>
              </response_label>
              <response_label ident="3206">
                <material>
                  <mattext texttype="text/plain">early stopping</mattext>
                </material>
              </response_label>
              <response_label ident="2998">
                <material>
                  <mattext texttype="text/plain">weight constraint</mattext>
                </material>
              </response_label>
            </render_choice>
          </response_lid>
        </presentation>
        <resprocessing>
          <outcomes>
            <decvar maxvalue="100" minvalue="0" varname="SCORE" vartype="Decimal"/>
          </outcomes>
          <respcondition continue="No">
            <conditionvar>
              <varequal respident="response1">3440</varequal>
            </conditionvar>
            <setvar action="Set" varname="SCORE">100</setvar>
          </respcondition>
        </resprocessing>
      </item>
      <item ident="g67b6a88022671f4c6777b17af76e5f08" title="ccc4602c-6daf-4002-ab69-19ae85b83d47">
        <itemmetadata>
          <qtimetadata>
            <qtimetadatafield>
              <fieldlabel>cc_profile</fieldlabel>
              <fieldentry>cc.multiple_choice.v0p1</fieldentry>
            </qtimetadatafield>
          </qtimetadata>
        </itemmetadata>
        <presentation>
          <material>
            <mattext texttype="text/html">&lt;div&gt;What are 2 examples of weight decay regularization?&lt;/div&gt;</mattext>
          </material>
          <response_lid ident="response1" rcardinality="Single">
            <render_choice>
              <response_label ident="7722">
                <material>
                  <mattext texttype="text/plain">L1 and L2</mattext>
                </material>
              </response_label>
              <response_label ident="2727">
                <material>
                  <mattext texttype="text/plain">MaxNorm and UnitNorm</mattext>
                </material>
              </response_label>
              <response_label ident="1222">
                <material>
                  <mattext texttype="text/plain">Adam and SGD</mattext>
                </material>
              </response_label>
              <response_label ident="9254">
                <material>
                  <mattext texttype="text/plain">Sigmoid and Softmax</mattext>
                </material>
              </response_label>
            </render_choice>
          </response_lid>
        </presentation>
        <resprocessing>
          <outcomes>
            <decvar maxvalue="100" minvalue="0" varname="SCORE" vartype="Decimal"/>
          </outcomes>
          <respcondition continue="No">
            <conditionvar>
              <varequal respident="response1">7722</varequal>
            </conditionvar>
            <setvar action="Set" varname="SCORE">100</setvar>
          </respcondition>
        </resprocessing>
      </item>
      <item ident="gc9aaf3cc8ce7656fc7ef24b0debcc50c" title="606cf9c2-1d4c-4f95-9100-849e27a63d4d">
        <itemmetadata>
          <qtimetadata>
            <qtimetadatafield>
              <fieldlabel>cc_profile</fieldlabel>
              <fieldentry>cc.multiple_choice.v0p1</fieldentry>
            </qtimetadatafield>
          </qtimetadata>
        </itemmetadata>
        <presentation>
          <material>
            <mattext texttype="text/html">&lt;div&gt;In which sequential Keras model method do you specify Callbacks such as EarlyStopping?&lt;/div&gt;</mattext>
          </material>
          <response_lid ident="response1" rcardinality="Single">
            <render_choice>
              <response_label ident="4986">
                <material>
                  <mattext texttype="text/plain">model.fit()</mattext>
                </material>
              </response_label>
              <response_label ident="9807">
                <material>
                  <mattext texttype="text/plain">model.add()</mattext>
                </material>
              </response_label>
              <response_label ident="9392">
                <material>
                  <mattext texttype="text/plain">model.compile()</mattext>
                </material>
              </response_label>
              <response_label ident="4158">
                <material>
                  <mattext texttype="text/plain">model.evaluate()</mattext>
                </material>
              </response_label>
            </render_choice>
          </response_lid>
        </presentation>
        <resprocessing>
          <outcomes>
            <decvar maxvalue="100" minvalue="0" varname="SCORE" vartype="Decimal"/>
          </outcomes>
          <respcondition continue="No">
            <conditionvar>
              <varequal respident="response1">4986</varequal>
            </conditionvar>
            <setvar action="Set" varname="SCORE">100</setvar>
          </respcondition>
        </resprocessing>
      </item>
      <item ident="gde61a1af3efde2d4ee60d3f4c6464ae3" title="f4d2d155-de21-458c-9338-3dd6e3ce654a">
        <itemmetadata>
          <qtimetadata>
            <qtimetadatafield>
              <fieldlabel>cc_profile</fieldlabel>
              <fieldentry>cc.multiple_choice.v0p1</fieldentry>
            </qtimetadatafield>
          </qtimetadata>
        </itemmetadata>
        <presentation>
          <material>
            <mattext texttype="text/html">&lt;div&gt;In the Module Project, to which layers did we apply weight decay (L1 or L2 regularization)?&lt;/div&gt;</mattext>
          </material>
          <response_lid ident="response1" rcardinality="Single">
            <render_choice>
              <response_label ident="3478">
                <material>
                  <mattext texttype="text/plain">Regularization is applied to the weights for all of the hidden layers.</mattext>
                </material>
              </response_label>
              <response_label ident="9908">
                <material>
                  <mattext texttype="text/plain">Only on hidden layers which have a certain number of nodes.</mattext>
                </material>
              </response_label>
              <response_label ident="3645">
                <material>
                  <mattext texttype="text/plain">To the output layer.</mattext>
                </material>
              </response_label>
              <response_label ident="2052">
                <material>
                  <mattext texttype="text/plain">To all of the hidden layers and the output layer.</mattext>
                </material>
              </response_label>
            </render_choice>
          </response_lid>
        </presentation>
        <resprocessing>
          <outcomes>
            <decvar maxvalue="100" minvalue="0" varname="SCORE" vartype="Decimal"/>
          </outcomes>
          <respcondition continue="Yes">
            <conditionvar>
              <other/>
            </conditionvar>
            <displayfeedback feedbacktype="Response" linkrefid="general_fb"/>
          </respcondition>
          <respcondition continue="Yes">
            <conditionvar>
              <varequal respident="response1">9908</varequal>
            </conditionvar>
            <displayfeedback feedbacktype="Response" linkrefid="9908_fb"/>
          </respcondition>
          <respcondition continue="Yes">
            <conditionvar>
              <varequal respident="response1">3645</varequal>
            </conditionvar>
            <displayfeedback feedbacktype="Response" linkrefid="3645_fb"/>
          </respcondition>
          <respcondition continue="Yes">
            <conditionvar>
              <varequal respident="response1">2052</varequal>
            </conditionvar>
            <displayfeedback feedbacktype="Response" linkrefid="2052_fb"/>
          </respcondition>
          <respcondition continue="No">
            <conditionvar>
              <varequal respident="response1">3478</varequal>
            </conditionvar>
            <setvar action="Set" varname="SCORE">100</setvar>
          </respcondition>
        </resprocessing>
        <itemfeedback ident="general_fb">
          <flow_mat>
            <material>
              <mattext texttype="text/plain">The reduce model complexity and prevent overfitting, we apply regularization to all the hidden layers in our model.</mattext>
            </material>
          </flow_mat>
        </itemfeedback>
        <itemfeedback ident="9908_fb">
          <flow_mat>
            <material>
              <mattext texttype="text/plain">The reduce model complexity and prevent overfitting, we apply regularization to all the hidden layers in our model.</mattext>
            </material>
          </flow_mat>
        </itemfeedback>
        <itemfeedback ident="3645_fb">
          <flow_mat>
            <material>
              <mattext texttype="text/plain">The output layer isn't regularized. The reduce model complexity and prevent overfitting, we apply regularization to all the hidden layers in our model.</mattext>
            </material>
          </flow_mat>
        </itemfeedback>
        <itemfeedback ident="2052_fb">
          <flow_mat>
            <material>
              <mattext texttype="text/plain">The output layer isn't regularized. The reduce model complexity and prevent overfitting, we apply regularization to all the hidden layers in our model.</mattext>
            </material>
          </flow_mat>
        </itemfeedback>
      </item>
      <item ident="gc836d2efdcf1ba6652988d75327511c8" title="d0d5776c-971e-45bd-bb32-0ef6d9fbe864">
        <itemmetadata>
          <qtimetadata>
            <qtimetadatafield>
              <fieldlabel>cc_profile</fieldlabel>
              <fieldentry>cc.multiple_choice.v0p1</fieldentry>
            </qtimetadatafield>
          </qtimetadata>
        </itemmetadata>
        <presentation>
          <material>
            <mattext texttype="text/html">&lt;div&gt;In the example model in the Module Project (&lt;code&gt;build_complex_model&lt;/code&gt;), what happens when we apply too much regularization (larger values of the penalty) to the model weights?&lt;/div&gt;</mattext>
          </material>
          <response_lid ident="response1" rcardinality="Single">
            <render_choice>
              <response_label ident="6169">
                <material>
                  <mattext texttype="text/html">The weights can decrease to zero which might limit the model's ability to learn.</mattext>
                </material>
              </response_label>
              <response_label ident="2233">
                <material>
                  <mattext texttype="text/html">The weight will grow too large, resulting in a model which can't generalize well to new data.</mattext>
                </material>
              </response_label>
              <response_label ident="6159">
                <material>
                  <mattext texttype="text/plain">The model will take a long time to learn and might not converge on a solution.</mattext>
                </material>
              </response_label>
              <response_label ident="9317">
                <material>
                  <mattext texttype="text/html">Too many neurons will be "turned off" in the dropout layer if the regularization penalty is too high.</mattext>
                </material>
              </response_label>
            </render_choice>
          </response_lid>
        </presentation>
        <resprocessing>
          <outcomes>
            <decvar maxvalue="100" minvalue="0" varname="SCORE" vartype="Decimal"/>
          </outcomes>
          <respcondition continue="Yes">
            <conditionvar>
              <other/>
            </conditionvar>
            <displayfeedback feedbacktype="Response" linkrefid="general_fb"/>
          </respcondition>
          <respcondition continue="Yes">
            <conditionvar>
              <varequal respident="response1">2233</varequal>
            </conditionvar>
            <displayfeedback feedbacktype="Response" linkrefid="2233_fb"/>
          </respcondition>
          <respcondition continue="Yes">
            <conditionvar>
              <varequal respident="response1">6159</varequal>
            </conditionvar>
            <displayfeedback feedbacktype="Response" linkrefid="6159_fb"/>
          </respcondition>
          <respcondition continue="Yes">
            <conditionvar>
              <varequal respident="response1">9317</varequal>
            </conditionvar>
            <displayfeedback feedbacktype="Response" linkrefid="9317_fb"/>
          </respcondition>
          <respcondition continue="No">
            <conditionvar>
              <varequal respident="response1">6169</varequal>
            </conditionvar>
            <setvar action="Set" varname="SCORE">100</setvar>
          </respcondition>
        </resprocessing>
        <itemfeedback ident="general_fb">
          <flow_mat>
            <material>
              <mattext texttype="text/plain">Regularization that is too aggressive (applies too much shrinkage to the weights) will result in a poor model.</mattext>
            </material>
          </flow_mat>
        </itemfeedback>
        <itemfeedback ident="2233_fb">
          <flow_mat>
            <material>
              <mattext texttype="text/plain">Regularization "shrinks" model weights so they don't grow too large. If the penalty is too small, the weight can still grow too large.</mattext>
            </material>
          </flow_mat>
        </itemfeedback>
        <itemfeedback ident="6159_fb">
          <flow_mat>
            <material>
              <mattext texttype="text/plain">The regularization term doesn't affect how long a model takes to train but insteads affects how well a model fits the data.</mattext>
            </material>
          </flow_mat>
        </itemfeedback>
        <itemfeedback ident="9317_fb">
          <flow_mat>
            <material>
              <mattext texttype="text/plain">The regularization penalty is applied to the model weight and does not create a dropout layer in a neural network.</mattext>
            </material>
          </flow_mat>
        </itemfeedback>
      </item>
      <item ident="gf15662d1c4e642866f419e7dac40c1c5" title="71bb5df3-53a5-4fe3-9e23-0e24520c384d">
        <itemmetadata>
          <qtimetadata>
            <qtimetadatafield>
              <fieldlabel>cc_profile</fieldlabel>
              <fieldentry>cc.multiple_choice.v0p1</fieldentry>
            </qtimetadatafield>
          </qtimetadata>
        </itemmetadata>
        <presentation>
          <material>
            <mattext texttype="text/html">&lt;div&gt;Why should we avoid using too high of a dropout probability in our neural network?&lt;/div&gt;</mattext>
          </material>
          <response_lid ident="response1" rcardinality="Single">
            <render_choice>
              <response_label ident="4291">
                <material>
                  <mattext texttype="text/plain">If there are not enough neurons remaining after dropout, the model will not be able to learn well.</mattext>
                </material>
              </response_label>
              <response_label ident="1329">
                <material>
                  <mattext texttype="text/plain">The model will be too complex and will overfit the data.</mattext>
                </material>
              </response_label>
              <response_label ident="481">
                <material>
                  <mattext texttype="text/plain">The model will take too long to converge on a solution.</mattext>
                </material>
              </response_label>
              <response_label ident="3352">
                <material>
                  <mattext texttype="text/plain">A high probability means that only a few neurons are dropped, resulting in little to no effect on the model results.</mattext>
                </material>
              </response_label>
            </render_choice>
          </response_lid>
        </presentation>
        <resprocessing>
          <outcomes>
            <decvar maxvalue="100" minvalue="0" varname="SCORE" vartype="Decimal"/>
          </outcomes>
          <respcondition continue="Yes">
            <conditionvar>
              <other/>
            </conditionvar>
            <displayfeedback feedbacktype="Response" linkrefid="general_fb"/>
          </respcondition>
          <respcondition continue="Yes">
            <conditionvar>
              <varequal respident="response1">1329</varequal>
            </conditionvar>
            <displayfeedback feedbacktype="Response" linkrefid="1329_fb"/>
          </respcondition>
          <respcondition continue="Yes">
            <conditionvar>
              <varequal respident="response1">481</varequal>
            </conditionvar>
            <displayfeedback feedbacktype="Response" linkrefid="481_fb"/>
          </respcondition>
          <respcondition continue="Yes">
            <conditionvar>
              <varequal respident="response1">3352</varequal>
            </conditionvar>
            <displayfeedback feedbacktype="Response" linkrefid="3352_fb"/>
          </respcondition>
          <respcondition continue="No">
            <conditionvar>
              <varequal respident="response1">4291</varequal>
            </conditionvar>
            <setvar action="Set" varname="SCORE">100</setvar>
          </respcondition>
        </resprocessing>
        <itemfeedback ident="general_fb">
          <flow_mat>
            <material>
              <mattext texttype="text/plain">Usually a dropout rate between 20-50% has good results, where 20% is a good starting point.</mattext>
            </material>
          </flow_mat>
        </itemfeedback>
        <itemfeedback ident="1329_fb">
          <flow_mat>
            <material>
              <mattext texttype="text/plain">A high dropout probability results in a less complex model that might under-fit instead of over-fit.</mattext>
            </material>
          </flow_mat>
        </itemfeedback>
        <itemfeedback ident="481_fb">
          <flow_mat>
            <material>
              <mattext texttype="text/plain">A high dropout probability will decrease the training time.</mattext>
            </material>
          </flow_mat>
        </itemfeedback>
        <itemfeedback ident="3352_fb">
          <flow_mat>
            <material>
              <mattext texttype="text/plain">A high dropout probability results in fewer neurons remaining in the dropout layer.</mattext>
            </material>
          </flow_mat>
        </itemfeedback>
      </item>
      <item ident="gba5108b0056d176083f3c06087b04ca9" title="7a709941-7387-4b1e-bd78-c295e4296feb">
        <itemmetadata>
          <qtimetadata>
            <qtimetadatafield>
              <fieldlabel>cc_profile</fieldlabel>
              <fieldentry>cc.multiple_choice.v0p1</fieldentry>
            </qtimetadatafield>
          </qtimetadata>
        </itemmetadata>
        <presentation>
          <material>
            <mattext texttype="text/html">&lt;div&gt;Which of the following choices will save both the weights and architecture of a model and then load the model after saving? Assume the model has been created and trained as &lt;code&gt;model&lt;/code&gt;.&lt;/div&gt;</mattext>
          </material>
          <response_lid ident="response1" rcardinality="Single">
            <render_choice>
              <response_label ident="6096">
                <material>
                  <mattext texttype="text/html">&lt;pre&gt;&lt;code&gt;model.save("my_model")
new_model = tf.keras.models.load_model("my_model")&lt;/code&gt;&lt;/pre&gt;</mattext>
                </material>
              </response_label>
              <response_label ident="4090">
                <material>
                  <mattext texttype="text/html">&lt;pre&gt;&lt;code&gt;cpoint = tf.keras.callbacks.ModelCheckpoint("weights_best.h5", verbose=1, save_weights_only=True)
model.load_weights("weights_best.h5")&lt;/code&gt;&lt;/pre&gt;</mattext>
                </material>
              </response_label>
              <response_label ident="8060">
                <material>
                  <mattext texttype="text/html">&lt;code&gt;new_model = tf.keras.models.load_model("my_model")&lt;/code&gt;</mattext>
                </material>
              </response_label>
              <response_label ident="3005">
                <material>
                  <mattext texttype="text/html">We can't save a model architecture, only the model weights.</mattext>
                </material>
              </response_label>
            </render_choice>
          </response_lid>
        </presentation>
        <resprocessing>
          <outcomes>
            <decvar maxvalue="100" minvalue="0" varname="SCORE" vartype="Decimal"/>
          </outcomes>
          <respcondition continue="Yes">
            <conditionvar>
              <other/>
            </conditionvar>
            <displayfeedback feedbacktype="Response" linkrefid="general_fb"/>
          </respcondition>
          <respcondition continue="Yes">
            <conditionvar>
              <varequal respident="response1">4090</varequal>
            </conditionvar>
            <displayfeedback feedbacktype="Response" linkrefid="4090_fb"/>
          </respcondition>
          <respcondition continue="Yes">
            <conditionvar>
              <varequal respident="response1">8060</varequal>
            </conditionvar>
            <displayfeedback feedbacktype="Response" linkrefid="8060_fb"/>
          </respcondition>
          <respcondition continue="Yes">
            <conditionvar>
              <varequal respident="response1">3005</varequal>
            </conditionvar>
            <displayfeedback feedbacktype="Response" linkrefid="3005_fb"/>
          </respcondition>
          <respcondition continue="No">
            <conditionvar>
              <varequal respident="response1">6096</varequal>
            </conditionvar>
            <setvar action="Set" varname="SCORE">100</setvar>
          </respcondition>
        </resprocessing>
        <itemfeedback ident="general_fb">
          <flow_mat>
            <material>
              <mattext texttype="text/plain">With the `keras.model.load_model` module, it's easy to load both the weight and architecture of a previously created and trained model.</mattext>
            </material>
          </flow_mat>
        </itemfeedback>
        <itemfeedback ident="4090_fb">
          <flow_mat>
            <material>
              <mattext texttype="text/plain">This will only save the model weights; we also want the architecture.</mattext>
            </material>
          </flow_mat>
        </itemfeedback>
        <itemfeedback ident="8060_fb">
          <flow_mat>
            <material>
              <mattext texttype="text/plain">We need to first save the model weight and architecture before loading the model.</mattext>
            </material>
          </flow_mat>
        </itemfeedback>
        <itemfeedback ident="3005_fb">
          <flow_mat>
            <material>
              <mattext texttype="text/plain">We can save a model architecture! This saves time when setting up a complex model that you might want to run on a another data set.</mattext>
            </material>
          </flow_mat>
        </itemfeedback>
      </item>
    </section>
  </assessment>
</questestinterop>
