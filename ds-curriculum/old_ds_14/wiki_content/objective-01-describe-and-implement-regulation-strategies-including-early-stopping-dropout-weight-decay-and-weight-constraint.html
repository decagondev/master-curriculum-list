<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<title>Objective 01 - Describe and Implement Regulation Strategies Including Early Stopping, Dropout, Weight Decay, and Weight Constraint</title>
<meta name="identifier" content="gdbeb7b013aa54b6e7737f6744af9c5c4"/>
<meta name="editing_roles" content="teachers"/>
<meta name="workflow_state" content="active"/>
</head>
<body>
<h2 id="overview">Overview</h2>
<p><span>In Unit 2, we applied regularization techniques to regression problems. In particular, we used ridge regression (also called L2 regression). In general, regularization introduces a penalty when the model parameters are fit. The penalty results in the model not learning the data "too well" and allows it to generalize better to new data.</span></p>
<h3 id="neural-network-regularization">Neural Network Regularization</h3>
<p><span>As we have seen in the first three modules in this sprint, neural networks are complicated. There are many large hyperparameters that need to be set. With a complex model, it's easy to overfit them. Fortunately, there are many regularization techniques that neural networks can apply.</span></p>
<p><span>While we can apply L2 regression to neural networks, there are other types of regularization that we'll look at first.</span></p>
<h3 id="regularization-techniques">Regularization Techniques</h3>
<ul>
<li>early stopping</li>
<li>dropout</li>
<li>weight decay</li>
<li>weight constraint</li>
</ul>
<h3 id="early-stopping">Early Stopping</h3>
<p><span>This method of regularization sounds just like the name: stop the training early to prevent overfitting. As the neural network trains, when the performance on the validation set begins to decrease, the training is stopped. The goal is to find the balance between learning the data well enough and overfitting.</span></p>
<h3 id="dropout">Dropout</h3>
<p><span>Using a method called dropout, we can take advantage of ensemble learning to reduce overfitting. By not using or randomly dropping nodes and their associated output from a layer in a network, we have created a "new" layer with different properties. This process results in a layer that learns a sparse representation and more randomness in the data; thus, it generalizes better to unseen data.</span></p>
<h3 id="weight-decay">Weight Decay</h3>
<p><span>To reduce the complexity of a model, we can add a term to the loss function. The idea of weight decay is to apply a term to the weights as they update during the gradient descent process. As a result, the weights decrease, which is why this process is called weight decay.</span></p>
<h3 id="weight-constraint">Weight Constraint</h3>
<p><span>Weight constraint is a similar regularization technique as weight decay, but instead of decreasing the weights, a predefined limit is set, and the weights aren't allowed to pass this limit. If they do, they are scaled to remain below the threshold.</span></p>
<p><span>It would be best not to use the regularization techniques or weight decay and weight constraint together; you need to choose one or the other.</span></p>
<p>Here are some general guidelines to follow:</p>
<ul>
<li>Always use EarlyStopping</li>
<li>Use EarlyStopping, Weight Decay and Dropout</li>
<li>Use EarlyStopping, Weight Constraint and Dropout</li>
</ul>
<h2 id="follow-along">Follow Along</h2>
<p><span>In the following example, we'll create a dataset that doesn't have a solution: the features are a sequence of integers, and the target is an array of zeros. Remember that a <em>callback</em> is an object that can perform actions at various stages of training. In this case, we'll use the EarlyStopping class and call it at the end of each epoch.</span></p>
<p><span>The monitored value is the loss; if the loss doesn't improve (decrease) after three epochs (patience=3), then the process will end "early" or before the ten epochs that the model training could complete if the loss were decreasing.</span></p>
<pre><code class="python language-python"># Example: Keras EarlyStopping documentation

# Imports
import tensorflow as tf
import numpy as np

# 
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
# This callback will stop the training when there is no improvement in  
# the validation loss for three consecutive epochs.  
model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss='mse')

# Create some data
# X: sequence from 0 to 99
X = np.arange(100).reshape(5, 20)

# y: an array of zeroes
y = np.zeros(5)

# Model this data using the EarlyStopping callback
history = model.fit(X, y, epochs=10, batch_size=1, callbacks=[callback])</code></pre>
<pre><code>Epoch 1/10
5/5 [==============================] - 0s 1ms/step - loss: 923942848003309568.0000
Epoch 2/10
5/5 [==============================] - 0s 1ms/step - loss: 3004282714424342087681259356553216.0000
Epoch 3/10
5/5 [==============================] - 0s 1ms/step - loss: inf
Epoch 4/10
5/5 [==============================] - 0s 1ms/step - loss: inf</code></pre>
<p>The training stopped after three epochs (not counting the first epoch). This example was completed with test data to force to the training to stop but the same general process as above can be applied to a real dataset.</p>
<h3 id="dropout-1">Dropout</h3>
<p>In this example, we'll use a small generated dataset, implement a dropout layers, and then look at the output.</p>
<pre><code class="python language-python"># Example: Keras regularization layers / Dropout layer

tf.random.set_seed(42)

# Drop 0.2 of the inputs (random)
layer = tf.keras.layers.Dropout(0.2, input_shape=(2,))
data = np.arange(10).reshape(5, 2).astype(np.float32)
print("The input data: ", data)

outputs = layer(data, training=True)
print("The output after applying a dropout:", outputs)</code></pre>
<pre><code>The input data:  [[0. 1.]
 [2. 3.]
 [4. 5.]
 [6. 7.]
 [8. 9.]]
The output after applying a dropout: tf.Tensor(
[[ 0.    1.25]
 [ 2.5   3.75]
 [ 0.    6.25]
 [ 7.5   8.75]
 [10.   11.25]], shape=(5, 2), dtype=float32)</code></pre>
<p>In the output, we can see that 1 out of 5 inputs have been set to zero, for this random seed.</p>
<h3 id="weight-regularization">Weight Regularization</h3>
<p>In the following example from the Keras documentation, we can see how to apply a weight constraint and a weight decay.</p>
<pre><code class="python language-python"># Weight constraint example
from tensorflow.keras.constraints import max_norm
from tensorflow.keras import layers

# kernel_constraint argument is for the main weights
model.add(layers.Dense(64, kernel_constraint=max_norm(2.)))

# Weight decay example
from tensorflow.keras import layers
from tensorflow.keras import regularizers

# Add in the weight "decay" terms
layer = layers.Dense(
    units=64,
    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)
)</code></pre>
<h2 id="challenge">Challenge</h2>
<p><span>Would you please select one of the above regularization techniques and apply it to a previous neural network example? For the weight regularization techniques, which may be more challenging to use, make sure to read through the documentation linked below first. Then give one of them a try!</span></p>
<h2 id="additional-resources">Additional Resources</h2>
<ul>
<li><a href="https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/">Early Stopping to Avoid Overtraining on Neural Network Models</a></li>
<li><a href="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/">Dropout for Regularizing Deep Neural Networks</a></li>
<li><a href="https://keras.io/api/callbacks/early_stopping/">Keras: EarlyStopping Class</a></li>
<li><a href="https://jmlr.org/papers/v15/srivastava14a.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting (paper)</a></li>
<li><a href="https://keras.io/api/layers/regularizers/">Keras: Layers and Regularizers</a></li>
<li><a href="https://keras.io/api/layers/constraints/">Keras: Layers and Constraints</a></li>
</ul>
</body>
</html>