<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DS Unit 4 Sprint 15: Major Neural Network Architectures</title>
    <link rel="stylesheet" href="../css/style.css">
</head>

<body>
    <header>
        <nav>
            <div class="logo">Data Science Unit 4</div>
            <ul>
                <li><a href="index.html" class="active">Home</a></li>
                <li class="dropdown">
                    <a href="#">Modules</a>
                    <div class="dropdown-content">
                        <a href="modules/module1/index.html">Module 1: Recurrent Neural Networks and LSTM</a>
                        <a href="modules/module2/index.html">Module 2: Convolutional Neural Networks</a>
                        <a href="modules/module3/index.html">Module 3: OpenAI and ChatGPT</a>
                        <a href="modules/module4/index.html">Module 4: Large Language Models</a>
                    </div>
                </li>
                <li><a href="code-alongs/index.html">Code-Alongs</a></li>
                <li><a href="sprint-challenge/index.html">Sprint Challenge</a></li>
            </ul>
        </nav>
    </header>

    <main class="container">
        <section id="welcome">
            <h1>DS Unit 4 Sprint 15: Major Neural Network Architectures</h1>
            <div class="content-box">
                <h2>Welcome to Sprint 15</h2>
                <p>Now that you've learned the foundations of Neural Networks, it's time to go deep! Of course, all
                    "deep learning" really means is "there's at least some hidden layers" - but there's a great deal of
                    variety both in the layered architecture and the behavior of individual "neurons" in the network.
                </p>

                <p>We'll study a few of the most effective recent innovations in neural networks and deep learning and
                    think a bit about what the future may hold. Is deep learning the path to artificial general
                    intelligence? Probably not - but LLMs might get us part way there!</p>
            </div>
        </section>

        <section id="sprint-overview">
            <div class="content-box">
                <h2>Sprint Modules</h2>
                <p>This sprint explores specialized neural network architectures that power today's most advanced AI
                    applications in computer vision, natural language processing, and time series forecasting:</p>

                <div class="module-cards">
                    <div class="module-card module1-accent">
                        <h3>Module 1</h3>
                        <h4>Recurrent Neural Networks and LSTM</h4>
                        <p>Traditional neural networks are feedforward, but recurrent neural networks introduce cycles
                            that act as memory. This makes them ideal for sequential data like natural language, time
                            series, and other contexts where order matters. Learn how LSTMs overcome the vanishing
                            gradient problem.</p>
                        <a href="modules/module1/index.html" class="btn">View Module</a>
                    </div>

                    <div class="module-card module2-accent">
                        <h3>Module 2</h3>
                        <h4>Convolutional Neural Networks</h4>
                        <p>CNNs draw from biological inspiration, using neuron connectivity patterns that resemble the
                            brain's visual fields. These models excel at image recognition and classification, often
                            achieving human-level performance with minimal preprocessing.</p>
                        <a href="modules/module2/index.html" class="btn">View Module</a>
                    </div>

                    <div class="module-card module3-accent">
                        <h3>Module 3</h3>
                        <h4>OpenAI and ChatGPT</h4>
                        <p>Explore ChatGPT and transformer-based models that demonstrate AI's ability to understand and
                            generate human-like text. Learn about the architecture, applications, and ethical
                            considerations of these powerful language models.</p>
                        <a href="modules/module3/index.html" class="btn">View Module</a>
                    </div>

                    <div class="module-card module4-accent">
                        <h3>Module 4</h3>
                        <h4>Large Language Models</h4>
                        <p>Work with LLMs through the OpenAI API and SDK for text generation and summarization. Learn to
                            customize outputs using parameters and set up local LLMs for specialized applications like
                            chatbots.</p>
                        <a href="modules/module4/index.html" class="btn">View Module</a>
                    </div>
                </div>
            </div>
        </section>

        <section id="resources">
            <div class="content-box">
                <h2>Sprint Resources</h2>
                <div class="resource-section">
                    <h3 class="resource-section-title">Primary Resources</h3>
                    <div class="resource-links">
                        <a href="code-alongs/index.html" class="resource-link">Code-Along Sessions</a>
                        <a href="sprint-challenge/index.html" class="resource-link">Sprint Challenge</a>
                    </div>
                </div>

                <div class="resource-section">
                    <h3 class="resource-section-title">Sequential Models and Time Series</h3>
                    <ul>
                        <li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank"
                                rel="noopener noreferrer">Understanding LSTM Networks</a></li>
                        <li><a href="https://distill.pub/2016/augmented-rnns/" target="_blank" rel="noopener noreferrer">Distill:
                                Attention and Augmented RNNs</a></li>
                        <li><a href="https://www.tensorflow.org/tutorials/text/text_generation" target="_blank"
                                rel="noopener noreferrer">TensorFlow: Text Generation with RNN</a></li>
                        <li><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank"
                                rel="noopener noreferrer">The Unreasonable Effectiveness of RNNs</a></li>
                    </ul>
                </div>

                <div class="resource-section">
                    <h3 class="resource-section-title">Computer Vision and CNNs</h3>
                    <ul>
                        <li><a href="https://cs231n.github.io/convolutional-networks/" target="_blank"
                                rel="noopener noreferrer">Stanford CS231n: Convolutional Neural Networks</a></li>
                        <li><a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53"
                                target="_blank" rel="noopener noreferrer">A Comprehensive Guide to CNNs</a></li>
                        <li><a href="https://distill.pub/2017/feature-visualization/" target="_blank"
                                rel="noopener noreferrer">Feature Visualization in Neural Networks</a></li>
                        <li><a href="https://keras.io/api/applications/" target="_blank" rel="noopener noreferrer">Keras
                                Applications: Pre-trained Models</a></li>
                    </ul>
                </div>

                <div class="resource-section">
                    <h3 class="resource-section-title">Large Language Models and OpenAI</h3>
                    <ul>
                        <li><a href="https://platform.openai.com/docs" target="_blank" rel="noopener noreferrer">OpenAI API
                                Documentation</a></li>
                        <li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">Attention Is All
                                You Need (Transformer Paper)</a></li>
                        <li><a href="https://huggingface.co/transformers/" target="_blank" rel="noopener noreferrer">Hugging Face
                                Transformers</a></li>
                        <li><a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener noreferrer">llama.cpp:
                                Local LLM Inference</a></li>
                    </ul>
                </div>

                <div class="resource-section">
                    <h3 class="resource-section-title">Advanced Deep Learning</h3>
                    <ul>
                        <li><a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener noreferrer">Deep Learning
                                Book</a></li>
                        <li><a href="https://playground.tensorflow.org/" target="_blank" rel="noopener noreferrer">TensorFlow
                                Playground</a></li>
                        <li><a href="https://blog.keras.io/building-autoencoders-in-keras.html" target="_blank"
                                rel="noopener noreferrer">Building Autoencoders in Keras</a></li>
                        <li><a href="https://distill.pub/" target="_blank" rel="noopener noreferrer">Distill: Interactive ML
                                Articles</a></li>
                    </ul>
                </div>
            </div>
        </section>
    </main>
</body>

</html>