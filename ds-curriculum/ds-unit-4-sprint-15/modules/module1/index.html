<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 1: Recurrent Neural Networks and LSTM</title>
    <link rel="stylesheet" href="../../assets/css/styles.css">
</head>
<body>
    <header>
        <nav>
            <div class="logo">Data Science Unit 4</div>
            <ul>
                <li><a href="../../index.html">Home</a></li>
                <li class="dropdown">
                    <a href="#">Modules</a>
                    <div class="dropdown-content">
                        <a href="../module1/index.html" class="active">Module 1: Recurrent Neural Networks and LSTM</a>
                        <a href="../module2/index.html">Module 2: Convolutional Neural Networks</a>
                        <a href="../module3/index.html">Module 3: AutoEncoders and Generative Models</a>
                        <a href="../module4/index.html">Module 4: Time Series Forecasting</a>
                    </div>
                </li>
                <li><a href="../../code-alongs/index.html">Code-Alongs</a></li>
                <li><a href="../../sprint-challenge/index.html">Sprint Challenge</a></li>
            </ul>
        </nav>
    </header>

    <main class="container">
        <section id="welcome">
            <h1>Module 1: Recurrent Neural Networks and LSTM</h1>
            <div class="card module1-accent">
                <h2>Module Overview</h2>
                <p>This module introduces Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, which are specialized architectures designed for processing sequential data. While the feed-forward neural networks we've explored previously work well for many tasks, they struggle with sequential data where the order and context matter. RNNs address this limitation by incorporating feedback loops that allow information to persist across time steps.</p>
                
                <p>You'll learn how RNNs process sequences by maintaining a "memory" of previous inputs, how the vanishing gradient problem limits traditional RNNs, and how LSTM networks overcome this limitation through specialized memory cells. By the end of this module, you'll be able to implement LSTM networks for text generation tasks using Keras, opening up possibilities for applications in natural language processing, time series analysis, and other sequence modeling domains.</p>
            </div>
        </section>

        <section id="learning-objectives">
            <h2>Learning Objectives</h2>
            <div class="content-box">
                <h3>1. Describe how Neural Networks are used for modeling sequences</h3>
                <ul>
                    <li>Define what sequences are and how they differ from other data types</li>
                    <li>Explain the limitations of feed-forward networks for sequential data</li>
                    <li>Describe the structure and function of recurrent neural networks (RNNs)</li>
                    <li>Explain the vanishing gradient problem in traditional RNNs</li>
                </ul>
            </div>

            <div class="content-box">
                <h3>2. Implement LSTM models for a text classification problem and a text generation problem</h3>
                <ul>
                    <li>Understand the architecture of LSTM networks and how they address the vanishing gradient problem</li>
                    <li>Implement character-level language models using LSTM networks</li>
                    <li>Process and prepare text data for sequence modeling</li>
                    <li>Generate new text sequences using trained LSTM models</li>
                </ul>
            </div>
        </section>

        <section id="guided-project">
            <h2>Guided Project</h2>
            <div class="content-box">
                <h3>Recurrent Neural Networks and LSTM Text Generation</h3>
                
                <div class="video-container">
                    <iframe class="wistia_embed" title="DS_431_RNN_and_LSTM_Lecture - Jupyter Notebook Video" src="https://fast.wistia.net/embed/iframe/j7ny5ngji7" width="640" height="360" name="wistia_embed" allow="autoplay; fullscreen" loading="lazy"></iframe>
                </div>
                
                <div class="resource-section">
                    <h3 class="resource-section-title">Project Resources</h3>
                    <div class="resource-links">
                        <a href="https://github.com/bloominstituteoftechnology/DS-Unit-4-Sprint-3-Deep-Learning/tree/main/module1-rnn-and-lstm" class="resource-link" target="_blank" rel="noopener">GitHub Repo</a>
                    </div>
                </div>
                
                <h4>Guided Project File:</h4>
                <p>DS_431_RNN_and_LSTM_Lecture.ipynb</p>
            </div>
        </section>

        <section id="module-assignment">
            <h2>Module Assignment</h2>
            <div class="content-box">
                <p>Please read the assignment file in the GitHub repository for detailed instructions on completing your assignment tasks.</p>
                
                <h4>Assignment File:</h4>
                <p>DS_431_RNN_and_LSTM_Assignment.ipynb</p>

                <p>In this assignment, you will build a Shakespeare Sonnet Generator using LSTM networks. Your tasks include:</p>
                <ul>
                    <li>Downloading and preprocessing Shakespeare's sonnets from Project Gutenberg</li>
                    <li>Cleaning and preparing text data for sequence modeling</li>
                    <li>Creating character sequences for LSTM model training</li>
                    <li>Building and training an LSTM model for text generation</li>
                    <li>Implementing character prediction and text generation functions</li>
                    <li>Testing your model by generating Shakespearean-style text from seed phrases</li>
                    <li>Analyzing how the model learns patterns from the training corpus</li>
                </ul>

                <h3>Assignment Solution Video</h3>
                <div class="video-container">
                    <iframe class="wistia_embed" title="DS_431_RNN_and_LSTM_Assignment - Jupyter Notebook Video" src="https://fast.wistia.net/embed/iframe/qxdotavixt" width="640" height="360" name="wistia_embed" allow="autoplay; fullscreen" loading="lazy"></iframe>
                </div>
            </div>
        </section>

        <section id="check-for-understanding">
            <h2>Check for Understanding</h2>
            <div class="content-box">
                <p>Complete the following items to test your understanding:</p>
                <ul>
                    <li>Explain the key differences between feed-forward neural networks and recurrent neural networks</li>
                    <li>Describe the vanishing gradient problem and how LSTM networks address it</li>
                    <li>Outline the process of preparing text data for sequence modeling with LSTMs</li>
                    <li>Explain how character-level text generation works with LSTM networks</li>
                    <li>Identify practical applications of LSTM networks beyond text generation</li>
                </ul>
            </div>
        </section>

        <section id="additional-resources">
            <h2>Additional Resources</h2>
            <div class="content-box">
                <div class="resource-section">
                    <h3 class="resource-section-title">Learning Resources</h3>
                    <div class="resource-links">
                        <a href="https://www.tensorflow.org/tutorials/text/text_generation" class="resource-link" target="_blank" rel="noopener">TensorFlow: Text Generation with RNN</a>
                        <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="resource-link" target="_blank" rel="noopener">Understanding LSTM Networks</a>
                        <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" class="resource-link" target="_blank" rel="noopener">The Unreasonable Effectiveness of RNNs</a>
                        <a href="https://distill.pub/2016/augmented-rnns/" class="resource-link" target="_blank" rel="noopener">Distill: Attention and Augmented RNNs</a>
                        <a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" class="resource-link" target="_blank" rel="noopener">Stanford CS230: RNN Cheatsheet</a>
                        <a href="https://machinelearningmastery.com/start-here/#lstm" class="resource-link" target="_blank" rel="noopener">Machine Learning Mastery: LSTM Resources</a>
                    </div>
                </div>
            </div>
        </section>
    </main>
</body>
</html> 