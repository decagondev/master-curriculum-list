<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 1: Recurrent Neural Networks and LSTM</title>
    <link rel="stylesheet" href="../../assets/css/styles.css">
</head>
<body>
    <header>
        <nav>
            <div class="logo">Data Science Unit 4</div>
            <ul>
                <li><a href="../../index.html">Home</a></li>
                <li class="dropdown">
                    <a href="#">Modules</a>
                    <div class="dropdown-content">
                        <a href="../module1/index.html" class="active">Module 1: Recurrent Neural Networks and LSTM</a>
                        <a href="../module2/index.html">Module 2: Convolutional Neural Networks</a>
                        <a href="../module3/index.html">Module 3: OpenAI and ChatGPT</a>
                        <a href="../module4/index.html">Module 4: Large Language Models</a>
                    </div>
                </li>
                <li><a href="../../code-alongs/index.html">Code-Alongs</a></li>
                <li><a href="../../sprint-challenge/index.html">Sprint Challenge</a></li>
            </ul>
        </nav>
    </header>

    <main class="container">
        <section id="welcome">
            <h1>Module 1: Recurrent Neural Networks and LSTM</h1>
            <div class="content-box">
                <h2>Module Overview</h2>
                <p>This module introduces Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, which are specialized architectures designed for processing sequential data. While the feed-forward neural networks we've explored previously work well for many tasks, they struggle with sequential data where the order and context matter. RNNs address this limitation by incorporating feedback loops that allow information to persist across time steps.</p>
                
                <p>You'll learn how RNNs process sequences by maintaining a "memory" of previous inputs, how the vanishing gradient problem limits traditional RNNs, and how LSTM networks overcome this limitation through specialized memory cells. By the end of this module, you'll be able to implement LSTM networks for text generation tasks using Keras, opening up possibilities for applications in natural language processing, time series analysis, and other sequence modeling domains.</p>
            </div>
        </section>

        <section id="learning-objectives">
            <div class="content-box">
                <h2>Learning Objectives</h2>
                <ul>
                    <li>Describe how Neural Networks are used for modeling sequences</li>
                    <li>Implement LSTM models for a text classification problem and a text generation problem</li>
                </ul>
            </div>
        </section>

        <section id="guided-project">
            <div class="content-box">
                <h2>Guided Project</h2>

                <p>Open <strong>DS_431_RNN_and_LSTM_Lecture.ipynb</strong> in the GitHub repository to follow along with the guided project.</p>

                <div class="resource-links">
                    <a href="https://github.com/bloominstituteoftechnology/DS-Unit-4-Sprint-3-Deep-Learning/tree/main/module1-rnn-and-lstm" class="resource-link primary" target="_blank" rel="noopener">GitHub Repo</a>
                </div>

                <div class="video-container">
                    <iframe class="wistia_embed" title="DS_431_RNN_and_LSTM_Lecture - Jupyter Notebook Video" src="https://fast.wistia.net/embed/iframe/j7ny5ngji7" width="640" height="360" name="wistia_embed" allow="autoplay; fullscreen" loading="lazy"></iframe>
                </div>
            </div>
        </section>

        <section id="module-assignment">
            <div class="content-box">
                <h2>Module Assignment</h2>
                <p>Build a Shakespeare Sonnet Generator using LSTM networks to create Shakespearean-style text from seed phrases.</p>
                
                <div class="resource-links">
                    <a href="https://github.com/bloominstituteoftechnology/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module1-rnn-and-lstm/DS_431_RNN_and_LSTM_Assignment.ipynb" class="resource-link" target="_blank" rel="noopener">Module 1 Assignment</a>
                </div>

                <h3>Assignment Solution Video</h3>
                <div class="video-container">
                    <iframe class="wistia_embed" title="DS_431_RNN_and_LSTM_Assignment - Jupyter Notebook Video" src="https://fast.wistia.net/embed/iframe/qxdotavixt" width="640" height="360" name="wistia_embed" allow="autoplay; fullscreen" loading="lazy"></iframe>
                </div>
            </div>
        </section>

        <section id="additional-resources">
            <div class="content-box">
                <h2>Additional Resources</h2>
                <h3>LSTM and RNN Fundamentals</h3>
                <ul>
                    <li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></li>
                    <li><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">The Unreasonable Effectiveness of RNNs</a></li>
                    <li><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" target="_blank" rel="noopener">Stanford CS230: RNN Cheatsheet</a></li>
                </ul>
                <h3>Text Generation and Implementation</h3>
                <ul>
                    <li><a href="https://www.tensorflow.org/tutorials/text/text_generation" target="_blank" rel="noopener">TensorFlow: Text Generation with RNN</a></li>
                    <li><a href="https://machinelearningmastery.com/start-here/#lstm" target="_blank" rel="noopener">Machine Learning Mastery: LSTM Resources</a></li>
                </ul>
                <h3>Advanced Topics</h3>
                <ul>
                    <li><a href="https://distill.pub/2016/augmented-rnns/" target="_blank" rel="noopener">Distill: Attention and Augmented RNNs</a></li>
                </ul>
            </div>
        </section>
    </main>
</body>
</html>
