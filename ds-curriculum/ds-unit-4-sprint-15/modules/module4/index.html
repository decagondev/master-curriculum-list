<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 4: Large Language Models</title>
    <link rel="stylesheet" href="../../assets/css/styles.css">
</head>

<body>
    <header>
        <nav>
            <div class="logo">Data Science Unit 4</div>
            <ul>
                <li><a href="../../index.html">Home</a></li>
                <li class="dropdown">
                    <a href="#" class="active">Modules</a>
                    <div class="dropdown-content">
                        <a href="../module1/index.html">Module 1: Recurrent Neural Networks and LSTM</a>
                        <a href="../module2/index.html">Module 2: Convolutional Neural Networks</a>
                        <a href="../module3/index.html">Module 3: OpenAI and ChatGPT</a>
                        <a href="../module4/index.html" class="active">Module 4: Large Language Models</a>
                    </div>
                </li>
                <li><a href="../../code-alongs/index.html">Code-Alongs</a></li>
                <li><a href="../../sprint-challenge/index.html">Sprint Challenge</a></li>
            </ul>
        </nav>
    </header>

    <main class="container">
        <section id="welcome">
            <h1>Module 4: Large Language Models</h1>
            <div class="content-box">
                <h2>Module Overview</h2>
                <p>This updated module takes you beyond interacting with existing LLM interfaces to building and
                    customizing your own LLM-powered applications. Building on the foundation from Module 3, you'll
                    learn to work directly with LLM APIs and local models to create sophisticated, context-aware
                    conversational agents.</p>

                <p>You'll explore how to design and implement local LLM bots with customizable prompts and parameters,
                    experiment with different model configurations, and tackle advanced challenges like implementing
                    memory systems for more coherent conversations. This hands-on approach will give you practical
                    experience in building production-ready LLM applications while understanding the technical
                    considerations involved in deploying these powerful models.</p>
            </div>
        </section>

        <section id="learning-objectives">
            <div class="content-box">
                <h2>Learning Objectives</h2>
                <ul>
                    <li>Develop and customize local LLM bots with parameterized prompts and configurations</li>
                    <li>Implement memory systems and context management for enhanced conversational experiences</li>
                </ul>
            </div>
        </section>

        <section id="guided-project">
            <div class="content-box">
                <h2>Guided Project</h2>

                <p>This guided project focuses on hands-on LLM implementation and does not have traditional repository
                    materials. For students interested in exploring additional technical background, you can review the
                    <a href="https://github.com/bloominstituteoftechnology/DS-Unit-4-Sprint-3-Deep-Learning/tree/main/module4-time-series"
                        target="_blank" rel="noopener noreferrer">legacy Time Series Forecasting material</a> as
                    supplementary
                    content, though the current guided project and assignment are the primary focus.
                </p>

                <h3>Building a Chatbot with Persitent Memory</h3>

                <div class="video-container">
                    <iframe class="wistia_embed" title="Working with Large Language Models Video"
                        src="https://fast.wistia.net/embed/iframe/ent2k5vi8x?seo=false&videoFoam=false" width="640"
                        height="360" name="wistia_embed" allow="fullscreen" loading="lazy"></iframe>
                </div>
            </div>
        </section>

        <section id="module-assignment">
            <div class="content-box">
                <h2>Module Assignment</h2>
                <p>This module features a hands-on implementation assignment that differs from our typical structured
                    exercises.</p>

                <h3>Building an Advanced Local LLM Bot</h3>

                <h4>Objective:</h4>
                <p>The main goal of this assignment is to develop a local LLM bot with customizable prompts and
                    parameters. As a stretch goal, you will implement a short-term memory model for the bot, allowing
                    for more coherent and context-aware interactions.</p>

                <p>The instructions for this project are intentionally a bit vague. The purpose of this is for you to
                    build something of your own design, which can present many challenges and more importantly, a
                    portfolio-worthy project.</p>

                <h4>Prerequisites:</h4>
                <ul>
                    <li>Python programming experience</li>
                    <li>Basic understanding of machine learning, NLP, and LLMs</li>
                    <li>Access to an LLM API or local LLM setup</li>
                </ul>

                <h4>Steps:</h4>
                <ol>
                    <li><strong>Initial Setup</strong>
                        <ul>
                            <li>Set up a basic bot using a local LLM or an API service.</li>
                        </ul>
                    </li>
                    <li><strong>Experimentation</strong>
                        <ul>
                            <li>Experiment with various prompts and parameters to understand their impact on the bot's
                                responses.</li>
                        </ul>
                    </li>
                    <li><strong>Refactoring</strong>
                        <ul>
                            <li>Refactor your bot into a function or class, making sure to parameterize the user_prompt.
                            </li>
                        </ul>
                    </li>
                    <li><strong>Memory Module (Stretch Goal)</strong>
                        <ul>
                            <li>Implement a memory system for your bot. This can range from simply feeding back previous
                                interactions to a more complex approach like a vector database for automatic relevant
                                recall.</li>
                        </ul>
                    </li>
                    <li><strong>Evaluation</strong>
                        <ul>
                            <li>Evaluate the performance in terms of coherence, relevance, and context-awareness.</li>
                        </ul>
                    </li>
                    <li><strong>Documentation</strong>
                        <ul>
                            <li>Document your design choices, implementation details, and observations.</li>
                        </ul>
                    </li>
                    <li><strong>Peer Review (Stretch Goal)</strong>
                        <ul>
                            <li>Share your project for peer review, focusing on the bot's design, performance, and
                                memory model.</li>
                        </ul>
                    </li>
                    <li><strong>Final Submission</strong>
                        <ul>
                            <li>Submit your code and documentation for evaluation.</li>
                        </ul>
                    </li>
                </ol>

                <h4>Evaluation Criteria:</h4>
                <ul>
                    <li>Quality of the design and implementation of the bot</li>
                    <li>Effectiveness of the parameterization and customization</li>
                    <li>Implementation and performance of the memory model (if attempted)</li>
                    <li>Peer review feedback (optional)</li>
                </ul>

                <h4>Resources:</h4>
                <ul>
                    <li><a href="https://platform.openai.com/docs/guides/text?api-mode=chat" target="_blank"
                            rel="noopener noreferrer">OpenAI API</a></li>
                    <li><a href="https://huggingface.co/models?pipeline_tag=text-generation&sort=trending"
                            target="_blank" rel="noopener noreferrer">Hugging Face LLMs</a></li>
                </ul>

                <h3>Assignment Solution Video</h3>
                <div class="video-container">
                    <iframe class="wistia_embed" title="LLM Bot Assignment Solution Video"
                        src="https://fast.wistia.net/embed/iframe/8psbb0unf3?seo=false&videoFoam=false" width="640"
                        height="360" name="wistia_embed" allow="fullscreen" loading="lazy"></iframe>
                </div>
            </div>
        </section>

        <section id="additional-resources">
            <div class="content-box">
                <h2>Additional Resources</h2>
                <h3>LLM APIs and Platforms</h3>
                <ul>
                    <li><a href="https://platform.openai.com/docs/api-reference" target="_blank"
                            rel="noopener noreferrer">OpenAI
                            API Reference</a></li>
                    <li><a href="https://huggingface.co/docs/transformers/main_classes/pipelines" target="_blank"
                            rel="noopener noreferrer">Hugging Face Transformers Pipelines</a></li>
                    <li><a href="https://docs.anthropic.com/claude/docs" target="_blank"
                            rel="noopener noreferrer">Anthropic Claude
                            API Documentation</a></li>
                    <li><a href="https://ai.google.dev/docs" target="_blank" rel="noopener noreferrer">Google AI
                            Platform
                            Documentation</a></li>
                </ul>
                <h3>Local LLM Implementation</h3>
                <ul>
                    <li><a href="https://ollama.ai/" target="_blank" rel="noopener noreferrer">Ollama: Run LLMs
                            Locally</a></li>
                    <li><a href="https://github.com/ggerganov/llama.cpp" target="_blank"
                            rel="noopener noreferrer">llama.cpp:
                            Efficient LLM Inference</a></li>
                    <li><a href="https://docs.llamaindex.ai/en/stable/" target="_blank"
                            rel="noopener noreferrer">LlamaIndex: Data
                            Framework for LLMs</a></li>
                    <li><a href="https://python.langchain.com/docs/get_started/introduction" target="_blank"
                            rel="noopener noreferrer">LangChain: Building Applications with LLMs</a></li>
                </ul>
                <h3>Memory and Context Management</h3>
                <ul>
                    <li><a href="https://github.com/chroma-core/chroma" target="_blank"
                            rel="noopener noreferrer">Chroma: Vector
                            Database for LLMs</a></li>
                    <li><a href="https://www.pinecone.io/learn/vector-database/" target="_blank"
                            rel="noopener noreferrer">Pinecone: Vector Database Guide</a></li>
                    <li><a href="https://python.langchain.com/docs/how_to/chatbots_memory/" target="_blank"
                            rel="noopener noreferrer">LangChain: How to Add Memory to Chatbots</a></li>
                </ul>
            </div>
        </section>
    </main>
</body>

</html>